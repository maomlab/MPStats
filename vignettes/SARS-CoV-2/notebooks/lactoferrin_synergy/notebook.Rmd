---
title: "Lactoferrin synergy analysis"
output: html_document
bibliography: bibliography.bibtex.bib
csl: apa.csl
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = 'center',
  cache = TRUE)

library(plyr)
library(tidyverse)

# for Bayesian modeling
library(brms)
library(bayesplot)
library(MPStats)
MPStats::bayesplot_theme()

# for laying out ggplot2 figures
library(patchwork)

```

```{r functions, echo=FALSE}

scale_x_infection_rate <- list(
  ggplot2::scale_x_continuous(
    "Percent of Cells in Well Infected",
    breaks = c(0, .2, .4, .6, .8, 1),
    labels = c("0", "20", "40", "60", "80", "100"),
    expand = c(.01,.01)))

scale_y_infection_rate <- list(
  ggplot2::scale_y_continuous(
    "Percent of Cells in Well Infected",
    breaks = c(0, .2, .4, .6, .8, 1),
    labels = c("0", "20", "40", "60", "80", "100"),
    expand = c(.01,.01)))

```

## Context
Is Lactoferrin synergistic with chloroquine, remdesivir, or proxalutamide?

### Bayesian Regression Workflow
Given a given dataset of treatments, each measured for a response at a range of doses, we will follow a Bayesian analysis workflow (@Gelman2020-sf, @Van_de_Schoot2020-ei), to build a series of regression models using tools from the Stan ecosystem (@Carpenter2017-pj, @Burkner2017-ww, @Vehtari2017-pw, @Gabry2017-jm, @Kay2018-kn, @Wickham2016-xy, @Wickham2019-jb, @Team2013-nq).  For each model we will,

1. Define and fit a probabilistic model, which combines a *prior* distribution over a set of parameters with the data to draw samples from *posterior* distribution over the parameters using Hamiltonian Markov Chain Monte Carlo.
2. Check for sampling convergence.
3. Use prior and posterior predictive checks to evaluate the model specification and fit.
4. Use cross validation to evaluate the generalizability of the model.
5. Assess inferences that can be made from the model.

Then, we will compare the models based on their fit of the data and inferences that can be made.



## Load data

First we will load the well scores
```{r data-paths, dependson=c("setup")}
plate_SARS_2020C_fname <- "../../raw_data/2020C_Remdesivir_LF_Well_Level.csv"
plate_SARS_2021A_fname <- "../../raw_data/2021A_Hydroxychloroquine_LF_Well_Level.csv"

```

```{r load-table, echo=FALSE, dependson=c("data-paths")}
data <- dplyr::bind_rows(
  readr::read_csv(plate_SARS_2020C_fname) %>%
    dplyr::transmute(
      plate_id = "SARS_2020C",
      well_id = Well_ID,
      condition = Condition,
      treatment_1 = "Lactoferrin",
      treatment_1_short = "Lf",
      treatment_1_concentration = Lactoferrin_Concentration,
      treatment_1_units = Lactoferrin_Units,
      treatment_2 = "Remdesivir",
      treatment_2_short = "Rem",
      treatment_2_concentration = Remdesivir_Concentration,
      treatment_2_units = Remdesivir_Units,
      infected_count = Infected_Cell_Count,
      total_count = Total_Cell_Count,
      infection_rate = Raw_Percent_Infected / 100,
      viability_rate = Percent_Viability / 100),
  readr::read_csv(plate_SARS_2021A_fname) %>%
    dplyr::filter(
      Transferrin_Concentration == 0, 
      Store_Bought_Lactoferrin_Concentration == 0) %>%
    dplyr::transmute(
      plate_id = `Plate ID`,
      well_id = Well_ID,
      condition = Condition,
      treatment_1 = "Lactoferrin",
      treatment_1_short = "Lf",
      treatment_1_concentration = Lactoferrin_Concentration,
      treatment_1_units = Lactoferrin_Units,
      treatment_2 = "Hydroxychloroquine",
      treatment_2_short = "HCQ",
      treatment_2_concentration = Hydroxychloroquine_Concentration,
      treatment_2_units = Hydroxychloroquine_Units,
      infected_count = Infected_Cell_Count,
      total_count = Total_Cell_Count,
      infection_rate = Raw_Percent_Infected / 100,
      viability_rate = Percent_Viability / 100)) %>%
  dplyr::mutate(
    log_dose_1 = log10(treatment_1_concentration),
    log_dose_2 = log10(treatment_2_concentration),
    row = MPStats::well_id_to_row(well_id),
    column = MPStats::well_id_to_column(well_id))

data_single <- dplyr::bind_rows(
  data %>%
    dplyr::filter(treatment_2_concentration == 0) %>%
    dplyr::transmute(
      plate_id,
      well_id,
      condition,
      treatment = treatment_1,
      treatment_concentration = treatment_1_concentration,
      treatment_units = treatment_1_units,
      infected_count,
      total_count,
      infection_rate,
      viability_rate,
      log_dose = log_dose_1),
  data %>%
    dplyr::filter(treatment_1_concentration == 0) %>%
    dplyr::transmute(
      plate_id,
      well_id,
      condition,
      treatment = treatment_2,
      treatment_concentration = treatment_2_concentration,
      treatment_units = treatment_2_units,
      infected_count,
      total_count,
      infection_rate,
      viability_rate,
      log_dose = log_dose_2))
```

## Data Summary
```{r function-plot-platemap, echo=FALSE}
plot_plate_map <- function(data) {
  ggplot2::ggplot(data = data %>% dplyr::mutate(row_label = LETTERS[row])) + 
    ggplot2::theme_bw() + 
    MPStats::geom_indicator(
      mapping = ggplot2::aes(indicator = condition),
      group = 1,
      size = 2,
      xpos = "left") +
    MPStats::geom_indicator(
      mapping = ggplot2::aes(indicator = paste0(treatment_1_short, " ", signif(treatment_1_concentration * 1e6, 3), " uM")),
      group = 2,
      size = 2,
      xpos = "left") +
    MPStats::geom_indicator(
      mapping = ggplot2::aes(indicator = paste0(treatment_2_short, " ", signif(treatment_2_concentration * 1e6, 3), " uM")),
      group = 3,
      size = 2,
      xpos = "left") + 
    MPStats::geom_indicator(
      mapping = ggplot2::aes(indicator = paste0(signif(infection_rate * 100, 3), " %")),
      group = 4,
      size = 2,
      xpos = "left") +
    ggplot2::facet_grid(row_label ~ column) +
    ggplot2::ggtitle(paste0("Plate Map: ", data$plate_id[1])) +
    ggplot2::theme()
}
```

Plate map for SARS_2020C
```{r plot-plate-map-2020C, dependson=c("load-data"), echo=FALSE, fig.height=8, fig.width=15, dev='pdf' }
plot_plate_map(data %>% dplyr::filter(plate_id == "SARS_2020C"))
```

```{r plot-plate-map-2021A, dependson=c("load-data"), echo=FALSE, fig.height=8, fig.width=15, dev='pdf' }
plot_plate_map(data %>% dplyr::filter(plate_id == "SARS_2021A"))
```


```{r data-summary, dependson=c("load-data"), echo=FALSE}
# e.g. check dose ranges are on the right scale
data %>%
  dplyr::group_by(treatment_1, treatment_2) %>%
  dplyr::summarize(
    dose_1_min = min(10^log_dose_1),
    dose_1_max = max(10^log_dose_1),
    dose_2_min = min(10^log_dose_2),
    dose_2_max = max(10^log_dose_2)) %>%
  dplyr::ungroup() %>%
  dplyr::transmute(
    `Treatment 1` = treatment_1,
    `Treatment 2` = treatment_2,
    `Min Dose1 (uM)` = signif(dose_1_min * 1e6, 3),
    `Max Dose1 (uM)` = signif(dose_1_max * 1e6, 3),
    `Min Dose2 (uM)` = signif(dose_2_min * 1e6, 3),
    `Max Dose2 (uM)` = signif(dose_2_max * 1e6, 3))

```

```{r data-summary-response, dependson=c("load-data"), echo=FALSE}
zscores <- data %>%
  dplyr::group_by(plate_id) %>%
  dplyr::do({
    plate_data <- .
    data.frame(
      plate_id = plate_data$plate_id[1],
      z_prime = MPStats::Zprime(
        positive = plate_data %>% dplyr::filter(condition == "PC") %>% magrittr::extract2("infection_rate"),
        negative = plate_data %>% dplyr::filter(condition == "NC") %>% magrittr::extract2("infection_rate")))
  })


ggplot2::ggplot(data = data) +
  ggplot2::theme_bw() +
  ggplot2::geom_density(
    mapping = ggplot2::aes(
      x = infection_rate,
      y = ..scaled..,
      fill = condition),
    alpha = .7,
    color = "black",
    size = .4) +
  ggplot2::geom_vline(
    data = data %>%
      dplyr::group_by(plate_id, condition) %>%
      dplyr::summarize(
        mean_infection_rate = mean(infection_rate),
        .groups = "keep") %>%
      dplyr::ungroup(),
    mapping = ggplot2::aes(
      xintercept = mean_infection_rate,
      color = condition)) +
  MPStats::geom_indicator(
    data = zscores,
    mapping = ggplot2::aes(indicator = paste0("Zprime: ", signif(z_prime, 3))),
    group = 1) +
  ggplot2::ggtitle("Distribution of scores by condtion type") +
  scale_x_infection_rate +
  ggplot2::facet_wrap(~plate_id, ncol=1)
```


# Fit single agent dose response models
```{r model-sigmoid4-plot_data, dependson=c("load-data"), message=FALSE, include=FALSE, refresh=-1, cache=TRUE, fig.height = 3, fig.width = 6}
dose_ranges <- data_single %>%
  dplyr::group_by(treatment) %>%
  dplyr::filter(log_dose > -Inf) %>%
  dplyr::summarize(
    min_log_dose = min(log_dose),
    max_log_dose = max(log_dose))


fit_data <- dplyr::bind_rows(
  tibble::tibble(treatment = "Lactoferrin", top = .18, bottom = .05, ic50 = -7, hill = -.2),
  tibble::tibble(treatment = "Remdesivir", top = .22, bottom = .05, ic50 = -9, hill = -.2),
  tibble::tibble(treatment = "Hydroxychloroquine", top = .13, bottom = .05, ic50 = -6, hill = -.2)) %>%
  dplyr::left_join(dose_ranges, by = c("treatment")) %>%
  dplyr::rowwise() %>%
  dplyr::do({
    params <- .
    tibble::tibble(
      treatment = params$treatment,
      log_dose = seq(params$min_log_dose, params$max_log_dose, length.out = 200),
      infection_rate = MPStats::generate_hill_effects(
        log_dose = log_dose,
        top = params$top, bottom = params$bottom, ic50 = params$ic50, hill = params$hill))
  })

ggplot2::ggplot(data_single %>% dplyr::filter(condition != "PC")) + 
  ggplot2::theme_bw() +
  ggplot2::geom_point(
    mapping = ggplot2::aes(
      x = log_dose,
      y = infection_rate,
      group = treatment)) +
  ggplot2::geom_line(
    data = fit_data,
    mapping = ggplot2::aes(
      x = log_dose,
      y = infection_rate)) +
  scale_y_infection_rate +
  ggplot2::scale_x_continuous("Log Dose", expand = c(0.1, 0.1)) +
  ggplot2::facet_wrap(~treatment, scales = "free_x") + 
  ggplot2::ggtitle("SARS-CoV-2 infection")

```



## Fit sigmoid4 regression model for the single agents
Here we fit a four-parameter sigmoid curve, with parameters `top`{.R}, `bottom`{.R}, `ic50`{.R} and `slope`{.R}. We'll use a non-conventional parameterization of the hill coefficient, so that actual slope is independent of the `(top-bottom)`{.R} range.
```{r model-single-sigmoid4, dependson=c("load-data"), message=FALSE, include=FALSE, refresh=-1, cache=TRUE}

model_single_sigmoid4 <- brms::brm(
  formula = brms::brmsformula(
    infection_rate ~ hill_model(log_dose, top, bottom, ic50, slope),
      top + bottom + ic50 + slope ~ 0 + treatment,
      nl = TRUE),
  stanvars = c(
    brms::stanvar(
      scode = paste(
        " real hill_model(",
        "   real log_dose,",
        "   real top,",
        "   real bottom,",
        "   real ic50,",
        "   real slope) {",
        "     if (log_dose == negative_infinity()) {",
        "       return top;",
        "     } else if( log_dose == positive_infinity()) {",
        "       return bottom;",
        "     } else {",
        "       return (top - bottom) * inv_logit(slope * 4 / top * (log_dose - ic50)) + bottom;",
        "     }",
        "}", sep = "\n"),
      block = "functions")),
    #brms::stanvar(scode = "  vector[] hill = b_slope * 4 / b_top;\n", block = "genquant", position = "end")),
  prior = c(
    brms::prior(normal(.2, .1), nlpar = "top"),
    brms::prior(normal(0, .1), nlpar = "bottom"),
    brms::prior(normal(-7, 3), nlpar = "ic50"),
    brms::prior(normal(-.2, .2), nlpar = "slope", ub=-.01)),
  inits=function(){
    list(
      b_top = as.array(rep(.15, 3)),
      b_bottom = as.array(rep(0, 3)),
      b_ic50 = as.array(c(-6, -7, -9)),
      b_slope = as.array(rep(-.2, 3)))},
  iter=8000,
  cores = 4,
  data = data_single %>% dplyr::filter(condition != "PC"),
  stan_model_args = list(
    verbose = FALSE))
model_single_sigmoid4$name <- "sigmoid4"
```
```{r model-sigmoid4-summary, dependson=c("model-sigmoid4"), echo=FALSE}
model_single_sigmoid4
```

### Check for convergence
Note that for less restrictive prior on the hill and top and bottom parameters, the top and bottom can switch, leading to low Rhat scores. This is easily detected in the traceplots.
```{r model-single-sigmoid4-traceplot, echo=FALSE, fig.width=10, fig.height=5}
model_single_sigmoid4 %>% MPStats::traceplot()
```
 
```{r model-single-sigmoid4-rankplot, dependson=c("model-single-sigmoid4"), echo=FALSE, fig.width=8, fig.height=3.5}
model_single_sigmoid4 %>% MPStats::rankplot()
```

### Check specification of priors
```{r model-single-sigmoid4-prior-posterior-draws, dependson=c("model-single-sigmoid4", "functions"), echo=FALSE, message=FALSE, results='hide'}
model_single_sigmoid4 %>% MPStats::prior_posterior_plot()
```


### Check quality of model fit
Leave-one-out cross validation
```{r model-single-sigmoid4-criteria, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
model_single_sigmoid4 %>% brms::expose_functions(., vectorize = TRUE, show_compiler_warnings = FALSE)
model_single_sigmoid4 <- model_single_sigmoid4 %>% 
  brms::add_criterion("loo") %>%
  brms::add_criterion("loo_R2")

#model_signel_sigmoid4 <- model_single_sigmoid4 %>%
#  brms::add_criterion(
#    criterion = "kfold",
#    folds = loo::kfold_split_grouped(
#      K = data$treatment %>% unique() %>% length(),
#      x = data$treatment))

model_single_sigmoid4$criteria$loo
```


Leave-one-drug-out cross validation
```{r model-single-sigmoid4-criteria-kfold, dependson=c("model-single-sigmoid4-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
#model_single_sigmoid4$criteria$kfold
```



```{r model-sigmoid4-ppc, dependson=c("model-single-sigmoid4-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
pp1 <- model_single_sigmoid4 %>% brms::pp_check("loo_pit_overlay", nsamples = 50)
pp2 <- model_single_sigmoid4 %>% brms::pp_check(nsamples = 50)
pp3 <- model_single_sigmoid4 %>% brms::pp_check(type='error_scatter_avg') +
  ggplot2::theme_bw() +
  ggplot2::coord_flip() +
  scale_y_infection_rate
pp3$layers <- c(
  ggplot2::geom_vline(xintercept = 0, color="grey20"),
  pp3$layers)

(pp1 | pp2) / pp3
```

```{r model-single-sigmoid4-pairs, dependson=c("model-sigmoid4", "functions"), dev="png", echo=FALSE}
model <- model_single_sigmoid4

plots <- model$prior %>%
  dplyr::group_by(coef) %>%
  dplyr::group_map({
    coef <- .$coef[1]
    model_single_sigmoid4 %>% MPStats::pairsplot(pars = coef)
  })
  
model_single_sigmoid4 %>% MPStats::pairsplot()
```

### Interpret the model
```{r regression-single-sigmoid4, dependson=c("model-sigmoid4", "functions"), echo=FALSE, fig.width=6, fig.height=3}

model <- model_single_sigmoid4

samples <- data_single %>%
  tidybayes::add_fitted_draws(
    model, n=100) %>%
  dplyr::ungroup()

ggplot2::ggplot(
  data_single %>%
    dplyr::filter(condition != "PC")) + 
  ggplot2::theme_bw() +
  ggplot2::geom_point(
    mapping = ggplot2::aes(
      x = log_dose,
      y = infection_rate,
      group = treatment)) +
  ggplot2::geom_line(
    data = samples,
    mapping = ggplot2::aes(
      x = log_dose,
      y = .value,
      group = .draw),
    alpha = .05,
    color = "blue") +
  scale_y_infection_rate +
  ggplot2::scale_x_continuous("Log Dose", expand = c(0.1, 0.1)) +
  ggplot2::facet_wrap(~treatment, scales = "free_x") + 
  ggplot2::ggtitle(paste0("SARS-CoV-2 infection: ", model$name))
```

## Fit binomial sigmoid4 regression model for the single agents
Here we fit a four-parameter sigmoid curve, with parameters `top`{.R}, `bottom`{.R}, `ic50`{.R} and `slope`{.R}. We'll use a non-conventional parameterization of the hill coefficient, so that actual slope is independent of the `(top-bottom)`{.R} range.
```{r model-single-bsigmoid4, dependson=c("load-data"), message=FALSE, include=FALSE, refresh=-1, cache=TRUE}

model_single_bsigmoid4_z <- brms::brm(
  formula = brms::brmsformula(
    infected_count | trials(total_count) ~ hill_model(log_dose, top, bottom, ic50, slope),
      top + bottom + ic50 + slope ~ 0 + treatment,
      nl = TRUE),
  family = binomial("identity"),
  stanvars = c(
    brms::stanvar(
      scode = paste(
        " real hill_model(",
        "   real log_dose,",
        "   real top,",
        "   real bottom,",
        "   real ic50,",
        "   real slope) {",
        "     if (log_dose == negative_infinity()) {",
        "       return top;",
        "     } else if( log_dose == positive_infinity()) {",
        "       return bottom;",
        "     } else {",
        "       return (top - bottom) * inv_logit(slope * 4 / top * (log_dose - ic50)) + bottom;",
        "     }",
        "}", sep = "\n"),
      block = "functions")),
    #brms::stanvar(scode = "  vector[] hill = b_slope * 4 / b_top;\n", block = "genquant", position = "end")),
  prior = c(
    brms::prior(normal(.2, .1), nlpar = "top"),
    brms::prior(normal(0, .1), nlpar = "bottom"),
    brms::prior(normal(-7, 3), nlpar = "ic50"),
    brms::prior(normal(-.2, .2), nlpar = "slope", ub=-.01)),
  inits=function(){
    list(
      b_top = as.array(rep(.15, 3)),
      b_bottom = as.array(rep(0, 3)),
      b_ic50 = as.array(c(-6, -7, -9)),
      b_slope = as.array(rep(-.2, 3)))},
  iter=8000,
  cores = 4,
  data = data_single %>% dplyr::filter(condition != "PC"),
  stan_model_args = list(
    verbose = FALSE))
model_single_bsigmoid4$name <- "bsigmoid4"



model_single_zbsigmoid4 <- brms::brm(
  formula = brms::brmsformula(
    infected_count | trials(total_count) ~ hill_model(log_dose, top, bottom, ic50, slope),
    top + bottom + ic50 + slope ~ 0 + treatment,
    nl = TRUE),
  family = zero_inflated_binomial(link="identity"),
  stanvars = c(
    brms::stanvar(
      scode = paste(
        " real hill_model(",
        "   real log_dose,",
        "   real top,",
        "   real bottom,",
        "   real ic50,",
        "   real slope) {",
        "     if (log_dose == negative_infinity()) {",
        "       return top;",
        "     } else if( log_dose == positive_infinity()) {",
        "       return bottom;",
        "     } else {",
        "       return (top - bottom) * inv_logit(slope * 4 / top * (log_dose - ic50)) + bottom;",
        "     }",
        "}", sep = "\n"),
      block = "functions")),
    #brms::stanvar(scode = "  vector[] hill = b_slope * 4 / b_top;\n", block = "genquant", position = "end")),
  prior = c(
    brms::prior(normal(.2, .1), nlpar = "top"),
    brms::prior(normal(0, .1), nlpar = "bottom"),
    brms::prior(normal(-7, 3), nlpar = "ic50"),
    brms::prior(normal(-.2, .2), nlpar = "slope", ub=-.01)),
  inits=function(){
    list(
      b_top = as.array(rep(.15, 3)),
      b_bottom = as.array(rep(0, 3)),
      b_ic50 = as.array(c(-6, -7, -9)),
      b_slope = as.array(rep(-.2, 3)))},
  iter=8000,
  cores = 4,
  data = data_single %>% dplyr::filter(condition != "PC"),
  stan_model_args = list(
    verbose = FALSE))
model_single_zbsigmoid4$name <- "zbsigmoid4"

```
```{r model-bsigmoid4-summary, dependson=c("model-sigmoid4"), echo=FALSE}
model_single_bsigmoid4
```

### Check for convergence
Note that for less restrictive prior on the hill and top and bottom parameters, the top and bottom can switch, leading to low Rhat scores. This is easily detected in the traceplots.
```{r model-single-bsigmoid4-traceplot, echo=FALSE, fig.width=10, fig.height=5}
model_single_bsigmoid4 %>% MPStats::traceplot()
```
 
```{r model-single-sigmoid4-rankplot, dependson=c("model-single-sigmoid4"), echo=FALSE, fig.width=8, fig.height=3.5}
model_single_sigmoid4 %>% MPStats::rankplot()
```

### Check specification of priors
```{r model-single-bsigmoid4-prior-posterior-draws, dependson=c("model-single-sigmoid4", "functions"), echo=FALSE, message=FALSE, results='hide'}
model_single_bsigmoid4 %>% MPStats::prior_posterior_plot()
```


### Check quality of model fit
Leave-one-out cross validation
```{r model-single-sigmoid4-criteria, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
model_single_zbsigmoid4 %>% brms::expose_functions(., vectorize = TRUE, show_compiler_warnings = FALSE)
model_single_zbsigmoid4 <- model_single_zbsigmoid4 %>% 
  brms::add_criterion("loo") %>%
  brms::add_criterion("loo_R2")

#model_signel_sigmoid4 <- model_single_sigmoid4 %>%
#  brms::add_criterion(
#    criterion = "kfold",
#    folds = loo::kfold_split_grouped(
#      K = data$treatment %>% unique() %>% length(),
#      x = data$treatment))

model_single_bsigmoid4$criteria$loo
```


Leave-one-drug-out cross validation
```{r model-single-sigmoid4-criteria-kfold, dependson=c("model-single-sigmoid4-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
#model_single_sigmoid4$criteria$kfold
```



```{r model-sigmoid4-ppc, dependson=c("model-single-sigmoid4-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
pp1 <- model_single_sigmoid4 %>% brms::pp_check("loo_pit_overlay", nsamples = 50)
pp2 <- model_single_sigmoid4 %>% brms::pp_check(nsamples = 50)
pp3 <- model_single_sigmoid4 %>% brms::pp_check(type='error_scatter_avg') +
  ggplot2::theme_bw() +
  ggplot2::coord_flip() +
  scale_y_infection_rate
pp3$layers <- c(
  ggplot2::geom_vline(xintercept = 0, color="grey20"),
  pp3$layers)

(pp1 | pp2) / pp3
```

```{r model-single-sigmoid4-pairs, dependson=c("model-sigmoid4", "functions"), dev="png", echo=FALSE}
model <- model_single_sigmoid4

plots <- model$prior %>%
  dplyr::group_by(coef) %>%
  dplyr::group_map({
    coef <- .$coef[1]
    model_single_sigmoid4 %>% MPStats::pairsplot(pars = coef)
  })
  
model_single_sigmoid4 %>% MPStats::pairsplot()
```

### Interpret the model
```{r regression-single-sigmoid4, dependson=c("model-sigmoid4", "functions"), echo=FALSE, fig.width=6, fig.height=3}

model <- model_single_sigmoid4

samples <- data_single %>%
  tidybayes::add_fitted_draws(
    model, n=100) %>%
  dplyr::ungroup()

ggplot2::ggplot(
  data_single %>%
    dplyr::filter(condition != "PC")) + 
  ggplot2::theme_bw() +
  ggplot2::geom_point(
    mapping = ggplot2::aes(
      x = log_dose,
      y = infection_rate,
      group = treatment)) +
  ggplot2::geom_line(
    data = samples,
    mapping = ggplot2::aes(
      x = log_dose,
      y = .value,
      group = .draw),
    alpha = .05,
    color = "blue") +
  scale_y_infection_rate +
  ggplot2::scale_x_continuous("Log Dose", expand = c(0.1, 0.1)) +
  ggplot2::facet_wrap(~treatment, scales = "free_x") + 
  ggplot2::ggtitle(paste0("SARS-CoV-2 infection: ", model$name))
```


# Fit Models

## Fit flat regression model
Now we will fit a flat regression model to use a model for no dependence of the response on the treatment

```{r model-flat, dependson=c("load-data"), message=FALSE, warning=FALSE, cache=TRUE, echo=TRUE, results='hide'}
model_flat <- brms::brm(
  formula = infection_rate ~ 0 + plate_id,
  prior = c(
    brms::prior(student_t(3, .5, 1),  class = "b"),
    brms::prior(student_t(3, 0, 1),  class = "sigma")),
  data = data %>% dplyr::filter(condition != "PC"),
  iter = 8000,
  cores = 4)
model_flat$name <- "flat"
```
Here is a summary of the model fit, at this point we're looking primarily for the quality of the simulation. There are three areas to consider.

   1. The Rhat should be close 1 indicating the chains are not exploring the same regions of parameter space and not getting stuck.
   2. The Bulk and Tail effective sample sizes (Bulk_ESS, Tail_ESS) should be > 1000.
   3. There are no divergences. The NUTs algorithm can detect when the parameter space is difficult to sample.
   
If these areas are not all satisfied, then either run the simulation for longer, or reparameterize the model. We'll return below to interpret model specification and parameter estimates.
```{r model-flat-summary, dependson=c("model-flat"), echo=FALSE}
model_flat %>% print(digits = 4)
```

### Check for convergence
To further check convergence we will evaluate the traceplot, which shows the sampled for each parameter and each chain across the MCMC trajectories. We're looking for convergence across the sampling.
```{r model-flat-traceplot, dependson=c("model-flat", "functions"), echo=FALSE, fig.width = 6, fig.height=4}
model_flat %>% MPStats::traceplot()
```

and the rank histogram. The idea here is that if the simulation has converged, then the rank of each sample should be uniform, which can be visually assessed by making a histogram.   
```{r model-flat-rankplot, dependson=c("model-flat", "functions"), echo=FALSE, fig.width = 6, fig.height=4}
model_flat %>% MPStats::rankplot()
```

### Check specification of priors
Well specified priors should incorporate any domain knowledge ranging from uninformative, weakly informative, to strongly informative. Further the extent the inferences are sensitive to the priors should be clear.

```{r model-flat-prior-posterior-draws, dependson=c("model-flat", "functions"), message = FALSE, warning = FALSE, echo = FALSE, results='hide', fig.width = 8, fig.height=2.5}
MPStats::prior_posterior_plot(model = model_flat, irq = .95)
```

### Check quality of model fit
To assess the model fit we will use leave-one-out cross validation, that is re-fitting the model for each (drug, dose) and measuring the posterior probability of the held out sample. This is a more reliable way to compare models and estimate how well the model will generalize, which we'll return to after we defined all the models. To make this more computationally tractable, we use a method called Pareto smoothed importance sampling to approximate it by re-sampling from the posterior distribution. To interpret these scores, we want the the expected log pointwise predictive density for a new dataset (`elpd_loo`{.R}) to be large, and `p_loo`{.R}, the p_loo is the difference between `elpd_loo`{.R} and the non-cross-validated log posterior predictive density to be small. `p_loo`{.R} can be interpreted as the effective number of parameters, and ideally it should be less than the total number of samples (`p_loo < N`{.R}) and less than the actual number of parameters `p_loo < p`{.R} where `p` is the number of parameters in the model. A rule of thumb for model selection is that all else being equal, the `elpd_loo`{.R} should be twice or four times the standard deviation better to prefer a different model.

An interesting feature of using leave-one-out cross validation, is it can be used detect outliers as `pareto_k`{.R} estimates for each point. If there were any, they would show up as values `> .5`{.R} or worse here.
```{r model-flat-criteria, dependson=c("model-flat"), message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
model_flat <- model_flat %>% brms::add_criterion("loo")
model_flat <- model_flat %>% brms::add_criterion("loo_R2")
# model_flat <- model_flat %>%
#   brms::add_criterion(
#     criterion = "kfold",
#     folds = loo::kfold_split_grouped(
#       K = data$treatment %>% unique() %>% length(),
#       x = data$treatment))
  
model_flat$criteria$loo
```

We can also estimate the leave-one-drug-out cross validation, where the question is how well the model generalizes to *new treatments*. 
```{r model-flat-criteria-kfold, dependson=c("model-flat-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
#model_flat$criteria$kfold
```

A way to visualize if there is model mis-specification is through a posterior predictive checks. The idea is that if the model fits the data, we should have a hard time distinguishing the data from fake data generated from the fit model. In the upper two plots, samples generated from the model are shown in thin blue lines and the actual data is the thick black line. In the upper left plot, the x-axis is the response in this case the response, and in the right, the x-axis has been streched and compressed so the samples from the model are approximately uniform (they dip down at the edges because of uncertainty in the model). For more details on posterior predictive checks and these plots see (Gabry2019-ra). Below, it shows the average prediction error as a function of the response. For the flat model, the further above or below the mid-line, the worse the error. 
```{r model-flat-ppc, dependson=c("model-flat-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
pp1 <- model_flat %>% brms::pp_check(nsamples = 50)
pp2 <- model_flat %>% brms::pp_check("loo_pit_overlay", nsamples = 50)
pp3 <- model_flat %>% brms::pp_check(type='error_scatter_avg') +
  ggplot2::theme_bw() +
  ggplot2::coord_flip() +
  scale_x_infection_rate
pp3$layers <- c(
  ggplot2::geom_vline(xintercept = 0, color="grey20"),
  pp3$layers)
(pp1 | pp2) / pp3
```

### Interpret the model
The pairs plot shows if there is any correlation in the among the parameters. This can help interpret the model, or indicate alternative models or paremterizations. This show very little correlation between the intercept (the b_ prefix comes from the brms framework).
```{r model-flat-pairs, dependson=c("model-flat", "functions"), dev="png", echo=FALSE}
model_flat %>% MPStats::pairsplot()
```

Now we will plot draws from the fitted model on scatter plot of the response vs treatment data  
```{r regression-flat, dependson=c("model-flat", "functions"), fig.width = 8, echo=FALSE}
p1 <- MPStats::plot_checkerboard_score_by_dose(
  treatment_scores = data %>%
    dplyr::filter(condition != "PC") %>% 
    dplyr::filter(plate_id == "SARS_2020C") %>%
    dplyr::transmute(
      dose1 = 10^log_dose_1,
      dose2 = 10^log_dose_2,
      score = infection_rate * 100),
    treatment_1_label = "Lactoferrin",
    treatment_2_label = "Remdesivir",
    treatment_1_units = "M",
    treatment_2_units = "M") +
  ggplot2::ggtitle("Lf vs. Rem") +
  viridis::scale_fill_viridis(
    "% Infected", 
    option = "cividis",
    guide = guide_colorbar(reverse = TRUE)) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 30, vjust = 1, hjust=1))

p2 <- MPStats::plot_checkerboard_score_by_dose(
  treatment_scores = data %>%
    dplyr::filter(plate_id == "SARS_2021A") %>%
    dplyr::filter(condition != "PC") %>% 
    dplyr::transmute(
      dose1 = 10^log_dose_1,
      dose2 = 10^log_dose_2,
      score = infection_rate * 100),
    treatment_1_label = "Lactoferrin",
    treatment_2_label = "Hydroxychloroquine",
    treatment_1_units = "M",
    treatment_2_units = "M") +
  ggplot2::ggtitle("Lf vs. HCQ" )  +
  viridis::scale_fill_viridis(
    "% Infected", 
    option = "cividis",
    guide = guide_colorbar(reverse = TRUE)) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 30, vjust = 1, hjust=1))

p1 + p2
```


## Fit MuSyC regression model
Now we will fit a MuSyC regression model to use a model for no dependence of the response on the treatment

```{r model-MuSyC, dependson=c("load-data"), message=FALSE, warning=FALSE, cache=TRUE, echo=TRUE, results='hide'}
model_MuSyC <-
  MPStats::fit_MuSyC_score_by_dose(
    well_scores = data,
    group_vars = vars(plate_id))

model_MuSyC <- brms::brm(
  formula = infection_rate ~ 0 + plate_id,
  prior = c(
    brms::prior(student_t(3, .5, 1),  class = "b"),
    brms::prior(student_t(3, 0, 1),  class = "sigma")),
  data = data %>% dplyr::filter(condition != "PC"),
  iter = 8000,
  cores = 4)
model_MuSyC$name <- "MuSyC"
```
Here is a summary of the model fit, at this point we're looking primarily for the quality of the simulation. There are three areas to consider.

   1. The Rhat should be close 1 indicating the chains are not exploring the same regions of parameter space and not getting stuck.
   2. The Bulk and Tail effective sample sizes (Bulk_ESS, Tail_ESS) should be > 1000.
   3. There are no divergences. The NUTs algorithm can detect when the parameter space is difficult to sample.
   
If these areas are not all satisfied, then either run the simulation for longer, or reparameterize the model. We'll return below to interpret model specification and parameter estimates.
```{r model-MuSyC-summary, dependson=c("model-MuSyC"), echo=FALSE}
model_MuSyC %>% print(digits = 4)
```

### Check for convergence
To further check convergence we will evaluate the traceplot, which shows the sampled for each parameter and each chain across the MCMC trajectories. We're looking for convergence across the sampling.
```{r model-MuSyC-traceplot, dependson=c("model-MuSyC", "functions"), echo=FALSE, fig.width = 6, fig.height=4}
model_MuSyC %>% MPStats::traceplot()
```

and the rank histogram. The idea here is that if the simulation has converged, then the rank of each sample should be uniform, which can be visually assessed by making a histogram.   
```{r model-MuSyC-rankplot, dependson=c("model-MuSyC", "functions"), echo=FALSE, fig.width = 6, fig.height=4}
model_MuSyC %>% MPStats::rankplot()
```

### Check specification of priors
Well specified priors should incorporate any domain knowledge ranging from uninformative, weakly informative, to strongly informative. Further the extent the inferences are sensitive to the priors should be clear.

```{r model-MuSyC-prior-posterior-draws, dependson=c("model-MuSyC", "functions"), message = FALSE, warning = FALSE, echo = FALSE, results='hide', fig.width = 8, fig.height=2.5}
MPStats::prior_posterior_plot(model = model_MuSyC, irq = .95)
```

### Check quality of model fit
To assess the model fit we will use leave-one-out cross validation, that is re-fitting the model for each (drug, dose) and measuring the posterior probability of the held out sample. This is a more reliable way to compare models and estimate how well the model will generalize, which we'll return to after we defined all the models. To make this more computationally tractable, we use a method called Pareto smoothed importance sampling to approximate it by re-sampling from the posterior distribution. To interpret these scores, we want the the expected log pointwise predictive density for a new dataset (`elpd_loo`{.R}) to be large, and `p_loo`{.R}, the p_loo is the difference between `elpd_loo`{.R} and the non-cross-validated log posterior predictive density to be small. `p_loo`{.R} can be interpreted as the effective number of parameters, and ideally it should be less than the total number of samples (`p_loo < N`{.R}) and less than the actual number of parameters `p_loo < p`{.R} where `p` is the number of parameters in the model. A rule of thumb for model selection is that all else being equal, the `elpd_loo`{.R} should be twice or four times the standard deviation better to prefer a different model.

An interesting feature of using leave-one-out cross validation, is it can be used detect outliers as `pareto_k`{.R} estimates for each point. If there were any, they would show up as values `> .5`{.R} or worse here.
```{r model-MuSyC-criteria, dependson=c("model-MuSyC"), message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
model_MuSyC <- model_MuSyC %>% brms::add_criterion("loo")
model_MuSyC <- model_MuSyC %>% brms::add_criterion("loo_R2")
# model_MuSyC <- model_MuSyC %>%
#   brms::add_criterion(
#     criterion = "kfold",
#     folds = loo::kfold_split_grouped(
#       K = data$treatment %>% unique() %>% length(),
#       x = data$treatment))
  
model_MuSyC$criteria$loo
```

We can also estimate the leave-one-drug-out cross validation, where the question is how well the model generalizes to *new treatments*. 
```{r model-MuSyC-criteria-kfold, dependson=c("model-MuSyC-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
#model_MuSyC$criteria$kfold
```

A way to visualize if there is model mis-specification is through a posterior predictive checks. The idea is that if the model fits the data, we should have a hard time distinguishing the data from fake data generated from the fit model. In the upper two plots, samples generated from the model are shown in thin blue lines and the actual data is the thick black line. In the upper left plot, the x-axis is the response in this case the response, and in the right, the x-axis has been streched and compressed so the samples from the model are approximately uniform (they dip down at the edges because of uncertainty in the model). For more details on posterior predictive checks and these plots see (Gabry2019-ra). Below, it shows the average prediction error as a function of the response. For the MuSyC model, the further above or below the mid-line, the worse the error. 
```{r model-MuSyC-ppc, dependson=c("model-MuSyC-criteria"), message=FALSE, warning=FALSE, echo=FALSE}
pp1 <- model_MuSyC %>% brms::pp_check(nsamples = 50)
pp2 <- model_MuSyC %>% brms::pp_check("loo_pit_overlay", nsamples = 50)
pp3 <- model_MuSyC %>% brms::pp_check(type='error_scatter_avg') +
  ggplot2::theme_bw() +
  ggplot2::coord_flip() +
  scale_x_infection_rate
pp3$layers <- c(
  ggplot2::geom_vline(xintercept = 0, color="grey20"),
  pp3$layers)
(pp1 | pp2) / pp3
```

### Interpret the model
The pairs plot shows if there is any correlation in the among the parameters. This can help interpret the model, or indicate alternative models or paremterizations. This show very little correlation between the intercept (the b_ prefix comes from the brms framework).
```{r model-MuSyC-pairs, dependson=c("model-MuSyC", "functions"), dev="png", echo=FALSE}
model_MuSyC %>% MPStats::pairsplot()
```

Now we will plot draws from the fitted model on scatter plot of the response vs treatment data  
```{r regression-MuSyC, dependson=c("model-MuSyC", "functions"), fig.width = 8, echo=FALSE}
p1 <- MPStats::plot_checkerboard_score_by_dose(
  treatment_scores = data %>%
    dplyr::filter(condition != "PC") %>% 
    dplyr::filter(plate_id == "SARS_2020C") %>%
    dplyr::transmute(
      dose1 = 10^log_dose_1,
      dose2 = 10^log_dose_2,
      score = infection_rate * 100),
    treatment_1_label = "Lactoferrin",
    treatment_2_label = "Remdesivir",
    treatment_1_units = "M",
    treatment_2_units = "M") +
  ggplot2::ggtitle("Lf vs. Rem") +
  viridis::scale_fill_viridis(
    "% Infected", 
    option = "cividis",
    guide = guide_colorbar(reverse = TRUE)) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 30, vjust = 1, hjust=1))

p2 <- MPStats::plot_checkerboard_score_by_dose(
  treatment_scores = data %>%
    dplyr::filter(plate_id == "SARS_2021A") %>%
    dplyr::filter(condition != "PC") %>% 
    dplyr::transmute(
      dose1 = 10^log_dose_1,
      dose2 = 10^log_dose_2,
      score = infection_rate * 100),
    treatment_1_label = "Lactoferrin",
    treatment_2_label = "Hydroxychloroquine",
    treatment_1_units = "M",
    treatment_2_units = "M") +
  ggplot2::ggtitle("Lf vs. HCQ" )  +
  viridis::scale_fill_viridis(
    "% Infected", 
    option = "cividis",
    guide = guide_colorbar(reverse = TRUE)) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 30, vjust = 1, hjust=1))

p1 + p2
```




## Compare all models
```{r gather-models, dependson=c("model-flat", "model-MuSyC"), echo=FALSE, results='hide'}
models <- tibble::tibble( model = list(
  model_flat,
  model_sigmoid4)) %>%
  dplyr::mutate(model_name = lapply(model, function(m){m$name}) %>% unlist(), .before = 1)

models %>%
  dplyr::rowwise() %>%
  dplyr::do({
    model_name <- .$model_name
    model <- .$model
    assertthat::assert_that(!is.null(model$name), msg = "Model name is null")
    assertthat::assert_that(!is.null(model$criteria$loo), msg = paste0("loo is null for model ", model_name))
    assertthat::assert_that(!is.null(model$criteria$loo_R2), msg = paste0("loo_R2 is null for model ", model_name))
    assertthat::assert_that(!is.null(model$criteria$kfold), msg = paste0("kfold is null for model ", model_name))
    data.frame()
    })
```

```{r regression-all-fits, dependson=c("gather-models"), echo=FALSE, fig.width = 10, fig.height = 6}

model_draws <- models %>%
   dplyr::rowwise() %>%
  dplyr::do({
    model <- .$model
    data %>% tidybayes::add_fitted_draws(
      model,
      n=40) %>%
      dplyr::mutate(model = model$name)
  }) %>%
  dplyr::ungroup()

format_R2 <- function(model){
  paste0(
    'R^2~"="~',
    model$criteria$loo_R2 %>% mean() %>% signif(2),
    '~" ["*', model$criteria$loo_R2 %>% quantile(0.025) %>% signif(2),
    '*", "*', model$criteria$loo_R2 %>% quantile(0.975) %>% signif(2),
    '*"]"')
}

R2_scores <- models %>%
  dplyr::transmute(
    label = lapply(model, format_R2),
    model = model_name)


args <- models$model
args <- append(args, values = list(criterion = "loo"))
args <- append(args, values = list(model_names = models$model_name))
names(args)[1] <- "x"
elpd_diff <- do.call(brms::loo_compare, args) %>%
  data.frame() %>%
  tibble::rownames_to_column(var = "model") %>%
  dplyr::transmute(
    model = model %>% stringr::str_replace("model_", ""),
    label = paste0('"elpd"["diff"]~"="~', signif(elpd_diff, 2), '~"+/-"~', signif(se_diff, 2)))


```


Compare based on leave-one-out cross validation
```{r model-compare-loo, dependson=c("gather-models"), echo=FALSE}
args <- models$model
args <- append(args, values = list(criterion = "loo"))
args <- append(args, values = list(model_names = models$model_name))
names(args)[1] <- "x"
do.call(brms::loo_compare, args)

```

Compare based on leave-drug-out cross validation
```{r model-compare-kfold, dependson=c("gather-models"), echo=FALSE}
args <- models$model
args <- append(args, values = list(criterion = "kfold"))
args <- append(args, values = list(model_names = models$model_name))
names(args)[1] <- "x"
do.call(brms::loo_compare, args)
```


```{r model-model-weights-kfold, dependson=c("gather-models"), echo=FALSE}
model_weights <- models$model %>%
  lapply(function(m){m$criteria$kfold$pointwise[,"elpd_kfold"]}) %>%
  do.call(cbind, .) %>%
  loo::stacking_weights()
attr(model_weights, "names") <- models$model_name
model_weights
```

# Session Info
For reproducibility, here is information about the R version and loaded packages
```{r session-info}
sessionInfo()
```

# References
